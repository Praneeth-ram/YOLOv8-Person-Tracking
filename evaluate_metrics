import os
import motmetrics as mm
import pandas as pd

# Define paths
GROUND_TRUTH_PATH = "data/MOT17/MOT17-04/gt/gt.txt"
PREDICTIONS_PATH = "data/logs/tracks.csv"

def evaluate_mot_metrics():
    if not os.path.exists(GROUND_TRUTH_PATH):
        raise FileNotFoundError(f"‚ùå Ground truth file not found: {GROUND_TRUTH_PATH}")
    if not os.path.exists(PREDICTIONS_PATH):
        raise FileNotFoundError(f"‚ùå Prediction file not found: {PREDICTIONS_PATH}")

    print("üì• Loading ground truth and predictions...")
    gt = mm.io.loadtxt(GROUND_TRUTH_PATH, fmt="mot15-2D")
    pred = mm.io.loadtxt(PREDICTIONS_PATH, fmt="mot15-2D")

    print("‚öôÔ∏è Comparing predictions to ground truth...")
    acc = mm.utils.compare_to_groundtruth(gt, pred, "iou", distth=0.5)

    mh = mm.metrics.create()
    summary = mh.compute(acc, metrics=["mota", "idf1", "num_switches", "num_matches", "num_false_positives"], name="overall")

    print("\nüìä MOT Metrics Summary:")
    print(mm.io.render_summary(summary, formatters=mh.formatters, namemap=mm.io.motchallenge_metric_names))

if __name__ == "__main__":
    evaluate_mot_metrics()
